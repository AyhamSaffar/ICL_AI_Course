{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Research Challenge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #f8d7da; border-left: 6px solid #ccc; margin: 20px; padding: 15px;\">\n",
    "    <strong>üí° Margaret Atwood:</strong> Every aspect of human technology has a dark side, including the bow and arrow.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üèÖ Build your own model \n",
    "\n",
    "You have been assigned one dataset from [MatBench](https://matbench.materialsproject.org) as introduced in the [Lecture slides](https://speakerdeck.com/aronwalsh/mlformaterials-challenge-25). You are free to choose and tune any machine-learning model, with any Python library, but it should be appropriate for the problem. For instance, [XGBoost](https://xgboost.readthedocs.io) could be a good starting starting point to build a regression model. You can refer back to earlier notebooks and repurpose code as needed. \n",
    "\n",
    "You may reach the limits of computing processing power on Google Colab. Building a useful model with limited resources is a real-world skill. Using other free resources is allowed if you find an alternative service, as is running on your own computer. A model tracker such as [wandb](https://wandb.ai) could be helpful for advanced users. If you want to try a brute force approach, a library such as [Automatminer](https://hackingmaterials.lbl.gov/automatminer) may be of interest.\n",
    "\n",
    "This notebook should be used for keeping a record of your model development, submission, and even your presentation. You are free to edit (add/remove/delete) or rearrange the cells as you see fit.\n",
    "\n",
    "### Your details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Insert your values\n",
    "Name = \"No Name\" # Replace with your name\n",
    "CID = 123446 # Replace with your College ID (as a numeric value with no leading 0s)\n",
    "\n",
    "# Set a random seed using the CID value\n",
    "CID = int(CID)\n",
    "np.random.seed(CID)\n",
    "\n",
    "# Print the message\n",
    "print(\"This is the work of \" + Name + \" [CID: \" + str(CID) + \"]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem statement\n",
    "\n",
    "You have been assigned one dataset from the [list](https://matbench.materialsproject.org/Benchmark%20Info/matbench_v0.1/) on [MatBench](https://matbench.materialsproject.org). You should state what problem you are trying to solve and comment on the best-performing model in the benchmark. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spare cell\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Data preparation\n",
    "\n",
    "Check the data distribution and apply appropriate pre-processing steps as required. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installation of libraries\n",
    "!pip install matminer # Datasets and featurisation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get dataset info from matminer\n",
    "from matminer.datasets import get_all_dataset_info\n",
    "from matminer.datasets import load_dataset\n",
    "\n",
    "# Uncomment the info line for your assigned challenge\n",
    "\n",
    "  # A (GTAs - Xia, Kinga)\n",
    "#info = get_all_dataset_info(\"matbench_dielectric\")\n",
    "\n",
    "  # B (GTAs - Irea, Pan)\n",
    "#info = get_all_dataset_info(\"matbench_expt_gap\")\n",
    "\n",
    "  # C (GTAs - Yifan, Fintan)\n",
    "#info = get_all_dataset_info(\"matbench_glass\")\n",
    "\n",
    "# Check the dataset information\n",
    "print(info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your dataset into a pandas DataFrame\n",
    "df = load_dataset(\" \")\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose relevant features, which may be based on composition or structure, depending on your problem. [matminer](https://hackingmaterials.lbl.gov/matminer/) is a good place to start."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model selection, testing and training \n",
    "\n",
    "Define your model and justify your choice based on the problem and available data. You can look back at earlier notebooks and investigate other examples online including in [scikit-learn](https://scikit-learn.org)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spare cell\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train, validate and test your model. Make sure to do proper data splits and to consider the hyperparamaters of your model.\n",
    "\n",
    "<details>\n",
    "<summary>Note on the ROC-AUC classification metric</summary>\n",
    "There is one metric we didn't cover but is used in Matbench. In binary classification models, the ROC-AUC (Receiver Operating Characteristic - Area Under the Curve) score can be used to evaluate performance. It quantifies the ability of the model to distinguish between positive and negative instances across different decision thresholds. A higher ROC-AUC score (ranging from 0.5 to 1) indicates better performance, with 1 representing a perfect classifier and 0.5 indicating performance no better than random chance. There is a more detailed discussion here: https://developers.google.com/machine-learning/crash-course/classification/roc-and-auc.\n",
    "\n",
    "The metric can be calculated using the `roc_auc_score` function from the `sklearn.metrics` module, e.g.\n",
    "\n",
    "```python\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Assuming you have true labels (y_true) and predicted probabilities (y_pred_prob) \n",
    "y_true = [...]  \n",
    "y_pred_prob = [...]  \n",
    "\n",
    "# Calculate ROC-AUC\n",
    "roc_auc = roc_auc_score(y_true, y_pred_prob)\n",
    "\n",
    "# Display the result\n",
    "print(f'ROC-AUC Score: {roc_auc:.4f}')\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spare cell\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model analysis and discussion\n",
    "\n",
    "How well does your final model perform? Think of metrics and plots that are useful to dig a little deeper. \n",
    "\n",
    "Compare against the best-performing model on the [MatBench](https://matbench.materialsproject.org) leaderboard.  With limited resources, don't expect to match this performance, but you should do better than a baseline model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spare cell\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Large Language Model (LLM) usage declaration\n",
    "\n",
    "Acknowledge use of a generative model during your assignment. Points to consider:\n",
    "\n",
    "* State which LLM (e.g. GPT-4, Gemini, Co-Pilot)\n",
    "\n",
    "* Specify tasks (e.g. summarising research or code snippets)\n",
    "\n",
    "* Were any limitations/biases noted?\n",
    "\n",
    "* How did you ensure ethical use?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spare cell\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚òòÔ∏è Final word\n",
    "\n",
    "Good luck building your own model! We hope that you enjoyed the course and exercises. Dive deeper into the aspects that caught your interest. A useful starting point may be the [Resources](https://aronwalsh.github.io/MLforMaterials/Resources.html) page. \n",
    "\n",
    "Remember that submission is on Blackboard and you should upload both the completed Juypter Notebook (`.ipynb` file), as well as your recorded narrated presentation (maximum 5 minutes; see guides on using [Zoom](https://www.youtube.com/watch?v=H9qhoAIzW3E) or [Powerpoint](https://www.youtube.com/watch?v=Y5dgwwa5XRA) for this purpose)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
