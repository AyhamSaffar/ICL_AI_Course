{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zhHRr3VVOMFm"
   },
   "source": [
    "# Classical Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cWJD84-9OMFo"
   },
   "source": [
    "<div style=\"background-color: #f8d7da; border-left: 6px solid #ccc; margin: 20px; padding: 15px;\">\n",
    "    <strong>💡 Hugh Cartwright:</strong> The tools of science are changing; artificial intelligence has spread to the laboratory.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vN4ra0MROMFp"
   },
   "source": [
    "<iframe class=\"speakerdeck-iframe\" frameborder=\"0\" src=\"https://speakerdeck.com/player/b098c15f50ce4a468a1c5eecd6de0f96\" title=\"Machine Learning for Materials (Lecture 5)\" allowfullscreen=\"true\" style=\"border: 0px; background-clip: padding-box; background-color: rgba(0, 0, 0, 0.1); margin: 0px; padding: 0px; border-radius: 6px; box-shadow: rgba(0, 0, 0, 0.2) 0px 5px 40px; width: 100%; height: auto; aspect-ratio: 560 / 420;\" data-ratio=\"1.3333333333333333\"></iframe>\n",
    "\n",
    "[Lecture slides](https://speakerdeck.com/aronwalsh/mlformaterials-lecture5-classical)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IPrAgT4POMFp"
   },
   "source": [
    "## 🎲 Metal or insulator?\n",
    "\n",
    "In life, some decisions are difficult to make. We hope that our experience informs a choice that is better than a random guess. The same is true for machine learning models.\n",
    "\n",
    "There are many situations where we want to classify materials according to their properties. One fundamental characteristic is whether a material is a metal or insulator. For this exercise, we can refer to these as class `0` and class `1` materials, respectively. \n",
    "\n",
    "From our general knowledge, Cu should be `0` and MgO should be `1`, but what about Tl<sub>2</sub>O<sub>3</sub> or Ni<sub>2</sub>Zn<sub>4</sub>?\n",
    "\n",
    "### Theoretical background\n",
    "\n",
    "Metals are characterised by their free electrons that facilitate the flow of electric current. This arises from a partially filled conduction band, allowing electrons to move easily when subjected to an electric field.\n",
    "\n",
    "Insulators are characterised by an occupied valence band and empty conduction band, impeding the flow of current. The absence of charge carriers hinders electrical conductivity, making them effective insulators of electricity. Understanding these fundamental differences is crucial for designing and optimising electronic devices.\n",
    "\n",
    "In this practical, we can use the electronic band gap of a material as a simple descriptor of whether it is a metal (E<sub>g</sub> = 0) or an insulator (E<sub>g</sub> > 0).\n",
    "\n",
    "$$\n",
    "E_g = E^{conduction-band}_{minimum} - E^{valence-band}_{maximum}\n",
    "$$\n",
    "\n",
    "This classification is coarse as we are ignoring the intermediate regime of semiconductors and more exotic behaviour such as superconductivity.\n",
    "\n",
    "![image](./images/5_bands.png)\n",
    "\n",
    "## $k$-means clustering\n",
    "\n",
    "Let's start by generating synthetic data for materials along with their class labels. To make the analysis faster and more illustrative, we can perform dimensionality reduction from a 10D to 2D feature space, and then cluster the data using $k$-means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CLEjvAiAOMFp",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Installation of libraries\n",
    "!pip install elementembeddings --quiet\n",
    "!pip install matminer --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import of modules\n",
    "import numpy as np  # Numerical operations\n",
    "import pandas as pd  # DataFrames\n",
    "import matplotlib.pyplot as plt  # Plotting\n",
    "import seaborn as sns  # Visualisation\n",
    "from sklearn.decomposition import PCA  # Principal component analysis (PCA)\n",
    "from sklearn.cluster import KMeans # k-means clustering\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix  # Model evaluation\n",
    "from sklearn.tree import DecisionTreeClassifier  # Decision tree classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Colab error solution</summary>\n",
    "If running the import module cell fails with an \"AttributeError\", click `Runtime` -> `Restart Session` and then simply rerun the cell.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uncorrelated data\n",
    "\n",
    "Pay attention to each step in the process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 0: Set the number of clusters\n",
    "n_clusters = 0\n",
    "\n",
    "# Step 1: Generating synthetic (random) data\n",
    "np.random.seed(42)\n",
    "num_materials = 200\n",
    "num_features = 10\n",
    "data = np.random.rand(num_materials, num_features)\n",
    "labels = np.random.randint(0, 2, num_materials)\n",
    "\n",
    "# Step 2: Reduce dimensions to 2 using PCA\n",
    "pca = PCA(n_components=2)\n",
    "reduced_data = pca.fit_transform(data)\n",
    "\n",
    "# Step 3: Cluster the data using k-means\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "predicted_labels = kmeans.fit_predict(reduced_data)\n",
    "\n",
    "# Step 4: Create a plot to visualise the clusters and known labels\n",
    "plt.figure(figsize=(5, 4))\n",
    "\n",
    "# Plot the materials labeled as metal (label=1)\n",
    "plt.scatter(reduced_data[labels == 1, 0], reduced_data[labels == 1, 1], c='lightblue', label='Metal')\n",
    "# Plot the materials labeled as insulator (label=0)\n",
    "plt.scatter(reduced_data[labels == 0, 0], reduced_data[labels == 0, 1], c='lightcoral', label='Insulator')\n",
    "# Plot the cluster centres as stars\n",
    "plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], c='gold', s=200, label='Cluster centres', marker='*')\n",
    "\n",
    "# Draw cluster boundaries\n",
    "h = 0.02  # step size for the meshgrid\n",
    "x_min, x_max = reduced_data[:, 0].min() - 1, reduced_data[:, 0].max() + 1\n",
    "y_min, y_max = reduced_data[:, 1].min() - 1, reduced_data[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "Z = kmeans.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "plt.contourf(xx, yy, Z, alpha=0.2, cmap='Pastel1')\n",
    "\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.title('$k$-means clustering of synthetic data')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iyjuZJYmOMFq"
   },
   "source": [
    "<details>\n",
    "<summary> Code hint </summary>\n",
    "The algorithm fails for 0 clusters. \n",
    "Increase the value of `n_clusters` and look at the behaviour.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RWChaS_mOMFr"
   },
   "source": [
    "The cluster centres are shown by yellow stars. The model doesn't perform well, as we just generated this \"materials data\" from random numbers. There are no correlations for the algorithms to exploit. Nonetheless, this type of \"failed experiment\" is common in real research.\n",
    "\n",
    "Since we know the labels, we can quantify how bad the model using the classification accuracy. Is it better than flipping a coin? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XyShX2J-OMFr",
    "outputId": "c3c1425f-4354-4080-c42d-f025bedca416",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Step 5: Quantify classification accuracy\n",
    "accuracy = accuracy_score(labels, predicted_labels)\n",
    "conf_matrix = confusion_matrix(labels, predicted_labels)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"\\nConfusion matrix:\")\n",
    "print(conf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uSM_A4-ZOMFr"
   },
   "source": [
    "## Decision tree classifier\n",
    "\n",
    "Let's see if we can do better using a dedicated classifier. We will now train a decision tree to tackle the same problem and visualise the decision boundary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 564
    },
    "id": "ZKbtozXuOMFr",
    "outputId": "1e23e4bb-e043-4c37-b74a-413a5e002bc1",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Step 0: Set the depth of the decision tree\n",
    "max_tree_depth = 0\n",
    "\n",
    "# Step 1: Train a decision tree classifier\n",
    "def train_decision_tree(depth, reduced_data, labels):\n",
    "    tree_classifier = DecisionTreeClassifier(max_depth=depth, random_state=42)\n",
    "    tree_classifier.fit(reduced_data, labels)\n",
    "    return tree_classifier\n",
    "\n",
    "tree_classifier = train_decision_tree(max_tree_depth, reduced_data, labels)\n",
    "predicted_labels = tree_classifier.predict(reduced_data)\n",
    "\n",
    "# Step 2: Create a plot to visualise the decision boundary of the decision tree\n",
    "plt.figure(figsize=(5, 4))\n",
    "\n",
    "# Plot the materials labeled as metal (label=1)\n",
    "plt.scatter(reduced_data[labels == 1, 0], reduced_data[labels == 1, 1], c='lightblue', label='Metal')\n",
    "# Plot the materials labeled as insulator (label=0)\n",
    "plt.scatter(reduced_data[labels == 0, 0], reduced_data[labels == 0, 1], c='lightcoral', label='Insulator')\n",
    "# Plot the decision boundary of the decision tree classifier\n",
    "h = 0.02  # step size for the meshgrid\n",
    "x_min, x_max = reduced_data[:, 0].min() - 1, reduced_data[:, 0].max() + 1\n",
    "y_min, y_max = reduced_data[:, 1].min() - 1, reduced_data[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "Z = tree_classifier.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "plt.contourf(xx, yy, Z, alpha=0.5, cmap='Pastel1')\n",
    "\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.title(f'Decision tree (max depth={max_tree_depth}) of synthetic data')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SW0VbC_4OMFr"
   },
   "source": [
    "<details>\n",
    "<summary> Code hint </summary>\n",
    "With no nodes, you have made an indecisive tree 🥁.\n",
    "    \n",
    "Increase the value of `max_tree_depth` and look at the behaviour.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sOqtQymnOMFs"
   },
   "source": [
    "There should be more structure in the decision boundary due to the more complex model, especially as you increase the tree depth.\n",
    "\n",
    "$k$-means clustering provides a simple way to group materials based on similarity, yielding a clear linear decision boundary. On the other hand, the decision tree classifier does better in handling non-linear separations. It constructs a boundary based on different feature thresholds, enabling it to capture fine-grained patterns. As always in ML, there is a balance of trade-offs between simplicity and accuracy.\n",
    "\n",
    "Is the decision tree more accurate? Let's see."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PucrDphBOMFs",
    "outputId": "09ad3aa5-1ee9-4597-e2ca-c38b5d85057b",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Step 3: Quantify classification accuracy\n",
    "accuracy = accuracy_score(labels, predicted_labels)\n",
    "conf_matrix = confusion_matrix(labels, predicted_labels)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(conf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SNQRw2RhOMFs"
   },
   "source": [
    "If you choose a large value for the tree depth, the decision tree will approach a perfect accuracy of 1.0. It does this by memorising (overfitting) the training data but is unlikely to generalise well to new (unseen) data, i.e. overfitting. In contrast, the accuracy of $k$-means clustering is lower because it is an unsupervised algorithm designed for clustering, not classification. Its performance depends on the data structure and the presence of distinct clusters in that feature space.\n",
    "\n",
    "### Correlated data\n",
    "\n",
    "Let's try again, but this time we will (manually) add some correlations into the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify dataset with correlation\n",
    "correlation_strength = 0.333\n",
    "for i in range(num_features):\n",
    "    # For some features, add a linear correlation with the labels\n",
    "    if i % 2 == 0:  # Correlate every other feature\n",
    "        data[:, i] = correlation_strength * labels + (1 - correlation_strength) * np.random.rand(num_materials)\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "reduced_data = pca.fit_transform(data)\n",
    "\n",
    "# Step 0: Set the depth of the decision tree\n",
    "max_tree_depth = 1\n",
    "\n",
    "# Step 1: Train a decision tree classifier\n",
    "def train_decision_tree(depth, reduced_data, labels):\n",
    "    tree_classifier = DecisionTreeClassifier(max_depth=depth, random_state=42)\n",
    "    tree_classifier.fit(reduced_data, labels)\n",
    "    return tree_classifier\n",
    "\n",
    "tree_classifier = train_decision_tree(max_tree_depth, reduced_data, labels)\n",
    "predicted_labels = tree_classifier.predict(reduced_data)\n",
    "\n",
    "# Step 2: Create a plot to visualise the decision boundary of the decision tree\n",
    "plt.figure(figsize=(5, 4))\n",
    "\n",
    "# Plot the materials labeled as metal (label=1)\n",
    "plt.scatter(reduced_data[labels == 1, 0], reduced_data[labels == 1, 1], c='lightblue', label='Metal')\n",
    "# Plot the materials labeled as insulator (label=0)\n",
    "plt.scatter(reduced_data[labels == 0, 0], reduced_data[labels == 0, 1], c='lightcoral', label='Insulator')\n",
    "# Plot the decision boundary of the decision tree classifier\n",
    "h = 0.02  # step size for the meshgrid\n",
    "x_min, x_max = reduced_data[:, 0].min() - 1, reduced_data[:, 0].max() + 1\n",
    "y_min, y_max = reduced_data[:, 1].min() - 1, reduced_data[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "Z = tree_classifier.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "plt.contourf(xx, yy, Z, alpha=0.5, cmap='Pastel1')\n",
    "\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.title(f'Decision tree (max depth={max_tree_depth}) for artificial materials')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Step 3: Quantify classification accuracy\n",
    "accuracy = accuracy_score(labels, predicted_labels)\n",
    "conf_matrix = confusion_matrix(labels, predicted_labels)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(conf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now even a very simple tree can effectively draw a decision boundary. Machine learning models take advantage of such correlations in high dimensional feature spaces. You can modify the correlation strength on line 2 to see the effect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yAJreGhfOMFs",
    "tags": []
   },
   "source": [
    "## Real materials\n",
    "\n",
    "We can save time again by making use of a pre-built dataset. We will return to [matminer](https://hackingmaterials.lbl.gov/matminer), which we used before, and load `matbench_expt_is_metal`.\n",
    "\n",
    "### Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matminer\n",
    "from matminer.datasets.dataset_retrieval import load_dataset\n",
    "\n",
    "# Use matminer to download the dataset\n",
    "df = load_dataset('matbench_expt_is_metal')\n",
    "print(f'The full dataset contains {df.shape[0]} entries. \\n')\n",
    "\n",
    "# Display the first 10 entries\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sXq9bXwGOMFs"
   },
   "source": [
    "<details>\n",
    "<summary> Code hint </summary>\n",
    "To load a different dataset, you simply change the name in 'load_dataset()'.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y76d3NLhZADO"
   },
   "source": [
    "### Materials featurisation\n",
    "\n",
    "Revisiting concepts from earlier Notebooks, featurising the chemical compositions is necessary to create a useful set of input vectors. This allows the presence (or absence) of an element (or element combinations) to act as a feature that the classifier takes account for.\n",
    "\n",
    "We will use [ElementEmbeddings](https://wmd-group.github.io/ElementEmbeddings) to featurise the `composition` column. The importance of the pooling method can be tested by generating two sets of features. In the first, the mean of the atomic vectors is used, while in the second, a max pooling method takes the maximum value of each component across all the atomic vectors in the composition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 559
    },
    "id": "sTJg5-4yY9au",
    "outputId": "18f140fe-b6c1-41dc-b1d2-3adfc6c40e73",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Featurise all chemical compositions\n",
    "from elementembeddings.composition import composition_featuriser\n",
    "\n",
    "# Compute element embeddings using mean and max pooling\n",
    "mean_df = composition_featuriser(df[\"composition\"], embedding=\"magpie\", stats=[\"mean\"])\n",
    "max_df = composition_featuriser(df[\"composition\"], embedding=\"magpie\", stats=[\"maxpool\"])\n",
    "\n",
    "# Convert \"is_metal\" column to integer labels (0, 1)\n",
    "df['is_metal'] = df['is_metal'].astype(int)\n",
    "mean_df['is_metal'] = df['is_metal']\n",
    "max_df['is_metal'] = df['is_metal']\n",
    "\n",
    "# Define feature matrices and target variable\n",
    "cols_to_drop = ['is_metal', 'formula']\n",
    "\n",
    "X_mean = mean_df.drop(columns=cols_to_drop, errors='ignore').values\n",
    "X_max = max_df.drop(columns=cols_to_drop, errors='ignore').values\n",
    "y = df['is_metal'].values  # Target variable\n",
    "\n",
    "# Preview first two rows \n",
    "print(\"Mean pooling features (first two rows, first 4 columns):\")\n",
    "print(mean_df.iloc[:2, :4])  \n",
    "print(\"\\nMax pooling features (first two rows, first 4 columns):\")\n",
    "print(max_df.iloc[:2, :4]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the output, you can see two numerical representations of the chemical compositions using different feature extraction techniques. Now let's see how they cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $k$-means clustering \n",
    "\n",
    "#### Mean pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform k-means clustering\n",
    "kmeans = KMeans(n_clusters=2, random_state=42)\n",
    "predicted_labels = kmeans.fit_predict(X_mean)\n",
    "\n",
    "# Adjust k-means output to match true labels\n",
    "if accuracy_score(y, predicted_labels) < 0.5:\n",
    "    predicted_labels = 1 - predicted_labels\n",
    "\n",
    "# Assess performance\n",
    "accuracy = accuracy_score(y, predicted_labels)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "conf_matrix = confusion_matrix(y, predicted_labels)\n",
    "\n",
    "plt.figure(figsize=(5, 4))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", \n",
    "            xticklabels=['Predicted Insulator', 'Predicted Metal'], \n",
    "            yticklabels=['True Insulator', 'True Metal'])\n",
    "plt.xlabel('Predicted label')\n",
    "plt.ylabel('True label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Max pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform k-means clustering\n",
    "kmeans = KMeans(n_clusters=2, random_state=42)\n",
    "predicted_labels = kmeans.fit_predict(X_max)\n",
    "\n",
    "# Adjust k-means output to match true labels\n",
    "if accuracy_score(y, predicted_labels) < 0.5:\n",
    "    predicted_labels = 1 - predicted_labels\n",
    "\n",
    "# Assess performance\n",
    "accuracy = accuracy_score(y, predicted_labels)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "conf_matrix = confusion_matrix(y, predicted_labels)\n",
    "\n",
    "plt.figure(figsize=(5, 4))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", \n",
    "            xticklabels=['Predicted Insulator', 'Predicted Metal'], \n",
    "            yticklabels=['True Insulator', 'True Metal'])\n",
    "plt.xlabel('Predicted label')\n",
    "plt.ylabel('True label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The difference in accuracy between the two methods for this simple example highlights the importance of choosing an appropriate pooling strategy when featurising materials data. In this case, mean pooling provides a more balanced representation, which better distinguishes between metals and insulators."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 🚨 Exercise 5\n",
    "\n",
    "<div style=\"background-color: #dceefb; border-left: 6px solid #ccc; margin: 20px; padding: 15px; border-radius: 5px;\">\n",
    "    <strong>💡 Coding exercises:</strong> The exercises are designed to apply what you have learned with room for creativity. It is fine to discuss solutions with your classmates, but the actual code should not be directly copied.\n",
    "</div>\n",
    "\n",
    "### Your details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Insert your values\n",
    "Name = \"No Name\" # Replace with your name\n",
    "CID = 123446 # Replace with your College ID (as a numeric value with no leading 0s)\n",
    "\n",
    "# Set a random seed using the CID value\n",
    "CID = int(CID)\n",
    "np.random.seed(CID)\n",
    "\n",
    "# Print the message\n",
    "print(\"This is the work of \" + Name + \" [CID: \" + str(CID) + \"]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4WAC3QJYOMFs",
    "tags": []
   },
   "source": [
    "### Problem\n",
    "\n",
    "The choice of featurisation method can significantly impact the performance of machine learning models, particularly in decision trees, which rely on the features to make accurate splits. \n",
    "\n",
    "Tasks will be given in class focusing on comparing the impact of different featurisation methods on classification performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Empty block for your answers\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Empty block for your answers\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary> Task hint </summary>\n",
    "For task 4, you can featurise a new composition using a command such as `new_material = composition_featuriser([\"AlGaN2\"], embedding=\"atomic\", stats=[\"sum\"])`\n",
    "</details>\n",
    "\n",
    "<div style=\"background-color: #d4edda; border-left: 6px solid #ccc; margin: 20px; padding: 15px; border-radius: 5px;\">\n",
    "    <strong>📓 Submission:</strong> When your notebook is complete in Google Colab, go to <em>File > Download</em> and choose <code>.ipynb</code>. The completed file should be uploaded to Blackboard under assignments for MATE70026.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 🌊 Dive deeper\n",
    "\n",
    "* _Level 1:_ Tackle Chapter 6 on Linear Two-Class Classification in [Machine Learning Refined](https://github.com/jermwatt/machine_learning_refined#what-is-new-in-the-second-edition).\n",
    "\n",
    "* _Level 2:_ Play [metal detection](http://palestrina.northwestern.edu/metal-detection/). Note, the website can be a little temperamental. \n",
    "\n",
    "* _Level 3:_ Dig deeper into the options for definitions decision trees and ensemble models in [scikit-learn](https://scikit-learn.org/stable/modules/tree.html)."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "vscode24",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
